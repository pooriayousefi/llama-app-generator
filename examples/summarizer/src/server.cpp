/*
 * Copyright 2025 Pooria Yousefi
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * @file server.cpp
 * @brief Example Application Server Using CRTP Pattern
 * 
 * Demonstrates how to create a custom application server by:
 * 1. Inheriting from AppServerBase<ExampleApp> (CRTP!)
 * 2. Implementing process_request() with your business logic
 * 3. Using get_llama_client() for AI integration
 * 
 * The CRTP pattern enables compile-time polymorphism with zero overhead!
 * 
 * Generated by llama-app-generator on {{DATE}}
 * 
 * @author Pooria Yousefi
 */

#include <iostream>
#include <signal.h>
#include "app_server_base.hpp"

using namespace pooriayousefi::llama::app;

// Forward declaration for signal handling
class ExampleApp;
ExampleApp* g_server = nullptr;

/**
 * @brief Signal handler for graceful shutdown
 */
void signal_handler(int signum)
{
    if (g_server)
    {
        std::cout << "\nShutting down server..." << std::endl;
        exit(signum);
    }
}

/**
 * @class ExampleApp
 * @brief Concrete application server using CRTP
 * 
 * Implements process_request() to handle:
 *  - chat: Interactive conversation with LLM
 *  - summarize: Text summarization using LLM
 *  - echo: Simple echo without LLM (for testing)
 */
class ExampleApp : public AppServerBase<ExampleApp>
{
public:
    /**
     * @brief Constructor - passes config to base class
     */
    ExampleApp(const RuntimeConfig& cfg) 
        : AppServerBase<ExampleApp>(cfg)
    {
    }
    
    /**
     * @brief Business logic implementation
     * 
     * NOTE: Not virtual! CRTP provides compile-time polymorphism.
     * 
     * @param request JSON request object with "action" field
     * @return JSON response
     */
    json process_request(const json& request)
    {
        std::cout << color::BLUE << "[ExampleApp] Processing request..." 
                  << color::RESET << std::endl;
        
        // Extract request data
        std::string action = request.value("action", "unknown");
        
        if (action == "chat")
        {
            return handle_chat(request);
        }
        else if (action == "summarize")
        {
            return handle_summarize(request);
        }
        else if (action == "echo")
        {
            return handle_echo(request);
        }
        else if (action == "complete")
        {
            return handle_complete(request);
        }
        else
        {
            return json{
                {"status", "error"},
                {"message", "Unknown action: " + action}
            };
        }
    }

private:
    /**
     * @brief Handle chat action
     * 
     * Interactive conversation with LLM.
     */
    json handle_chat(const json& request)
    {
        std::string user_message = request.value("message", "");
        
        if (user_message.empty())
        {
            return {
                {"status", "error"},
                {"message", "message field is required"}
            };
        }
        
        try
        {
            // Use internal LlamaClient to get AI response
            json llm_response = get_llama_client()->complete(user_message, 512, 0.7);
            
            return {
                {"status", "success"},
                {"action", "chat"},
                {"user_message", user_message},
                {"ai_response", llm_response["content"]}
            };
        }
        catch (const std::exception& e)
        {
            return {
                {"status", "error"},
                {"message", std::string("LLM error: ") + e.what()}
            };
        }
    }
    
    /**
     * @brief Handle summarize action
     * 
     * Text summarization using LLM.
     */
    json handle_summarize(const json& request)
    {
        std::string text = request.value("text", "");
        
        if (text.empty())
        {
            return {
                {"status", "error"},
                {"message", "text field is required"}
            };
        }
        
        try
        {
            // Build summarization prompt
            std::string prompt = "Summarize the following text concisely:\n\n" + text;
            json llm_response = get_llama_client()->complete(prompt, 256, 0.5);
            
            return {
                {"status", "success"},
                {"action", "summarize"},
                {"original_length", text.length()},
                {"summary", llm_response["content"]}
            };
        }
        catch (const std::exception& e)
        {
            return {
                {"status", "error"},
                {"message", std::string("LLM error: ") + e.what()}
            };
        }
    }
    
    /**
     * @brief Handle complete action
     * 
     * Simple text completion.
     */
    json handle_complete(const json& request)
    {
        std::string prompt = request.value("prompt", "");
        
        if (prompt.empty())
        {
            return {
                {"status", "error"},
                {"message", "prompt field is required"}
            };
        }
        
        try
        {
            json llm_response = get_llama_client()->complete(prompt, 512, 0.7);
            
            return {
                {"status", "success"},
                {"action", "complete"},
                {"prompt", prompt},
                {"completion", llm_response["content"]}
            };
        }
        catch (const std::exception& e)
        {
            return {
                {"status", "error"},
                {"message", std::string("LLM error: ") + e.what()}
            };
        }
    }
    
    /**
     * @brief Handle echo action (no LLM needed)
     * 
     * Simple echo for testing connectivity.
     */
    json handle_echo(const json& request)
    {
        std::string message = request.value("message", "");
        
        return {
            {"status", "success"},
            {"action", "echo"},
            {"message", message}
        };
    }
};

/**
 * @brief Main entry point
 */
int main(int argc, char* argv[])
{
    std::string config_path = (argc >= 2) ? argv[1] : "config.txt";
    
    try
    {
        // Load configuration
        RuntimeConfig config = RuntimeConfig::from_file(config_path);
        
        std::cout << color::GREEN << "==================================" << color::RESET << std::endl;
        std::cout << color::GREEN << "  {{PROJECT_NAME}} Server" << color::RESET << std::endl;
        std::cout << color::GREEN << "  namespace: pooriayousefi::llama::app" << color::RESET << std::endl;
        std::cout << color::GREEN << "==================================" << color::RESET << std::endl;
        std::cout << "LLM Server: " << config.get_llama_server_url() << std::endl;
        std::cout << "API Port:   " << config.app_server_port << std::endl;
        std::cout << std::endl;
        
        // Validate configuration
        config.validate();
        
        // Create server instance (CRTP in action!)
        ExampleApp app(config);
        g_server = &app;
        
        // Setup signal handlers
        signal(SIGINT, signal_handler);
        signal(SIGTERM, signal_handler);
        
        // Start server
        std::cout << color::YELLOW << "Starting server on port " 
                  << config.app_server_port << "..." << color::RESET << std::endl;
        std::cout << color::BLUE << "Available endpoints:" << color::RESET << std::endl;
        std::cout << "  GET  /health  - Health check" << std::endl;
        std::cout << "  POST /api     - Main API" << std::endl;
        std::cout << std::endl;
        std::cout << color::BLUE << "API Actions:" << color::RESET << std::endl;
        std::cout << "  {\"action\": \"complete\", \"prompt\": \"...\"}" << std::endl;
        std::cout << "  {\"action\": \"chat\", \"message\": \"...\"}" << std::endl;
        std::cout << "  {\"action\": \"summarize\", \"text\": \"...\"}" << std::endl;
        std::cout << "  {\"action\": \"echo\", \"message\": \"...\"}" << std::endl;
        std::cout << std::endl;
        std::cout << color::YELLOW << "Press Ctrl+C to stop" << color::RESET << std::endl;
        std::cout << std::endl;
        
        app.start();
        
        return 0;
    }
    catch (const std::exception& e)
    {
        std::cerr << color::RED << "âœ— Error: " << e.what() << color::RESET << std::endl;
        return 1;
    }
}
