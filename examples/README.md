# llama-app-generator Examples

This directory contains practical examples demonstrating different use cases for applications generated by **llama-app-generator**.

## Overview

Each example showcases a complete application with:
- **Application Server**: Connects to llama-server and provides domain-specific API
- **CLI Client**: Command-line interface for testing and interaction
- **Configuration**: Runtime configuration via `config.txt`
- **Build Scripts**: Cross-platform Python build system with progress indicators

All examples use the **pooriayousefi::llama::app** namespace and are licensed under Apache License 2.0.

---

## Architecture

```
┌─────────────┐         ┌──────────────────┐         ┌────────────┐
│ llama-server│◄────────┤ Application      │◄────────┤ CLI Client │
│ (port 8080) │  HTTP   │ Server (port X)  │  HTTP   │            │
└─────────────┘         └──────────────────┘         └────────────┘
                              │
                              ├─ Uses LlamaClient (cpp-httplib)
                              ├─ Inherits AppServerBase<T> (CRTP)
                              └─ Domain-specific logic
```

### Key Components

1. **LlamaClient**: HTTP client for communicating with llama-server
2. **AppServerBase<T>**: CRTP base class providing server infrastructure
3. **RuntimeConfig**: Configuration parser with environment variable expansion
4. **Domain Server**: Custom application logic (e.g., ChatbotServer, SummarizerServer)
5. **CLI Client**: User-facing interface with domain-specific commands

---

## Examples

### 1. Chatbot (`chatbot/`)

**Description**: Interactive conversational AI application

**Features**:
- Real-time chat interface
- Conversation history tracking
- Streaming text generation

**Usage**:
```bash
cd chatbot
python3 build.py
./bin/server      # Start server on port 8081
./bin/client chat "Hello! How are you?"
```

**API Endpoints**:
- `POST /chat` - Send message and receive response

**Use Case**: Customer support bots, virtual assistants, interactive help systems

---

### 2. Summarizer (`summarizer/`)

**Description**: Text summarization service

**Features**:
- Multi-format support (articles, documents, emails)
- Configurable summary length
- Batch processing capability

**Usage**:
```bash
cd summarizer
python3 build.py
./bin/server      # Start server on port 8082
./bin/client summarize "Your long text here..."
```

**API Endpoints**:
- `POST /summarize` - Generate summary of provided text

**Use Case**: Document processing, email summarization, content aggregation

---

### 3. Code Assistant (`code-assistant/`)

**Description**: Programming assistance and code completion

**Features**:
- Code completion and suggestions
- Multi-language support
- Context-aware responses

**Usage**:
```bash
cd code-assistant
python3 build.py
./bin/server      # Start server on port 8083
./bin/client complete "def factorial(n):"
```

**API Endpoints**:
- `POST /complete` - Generate code completion
- `POST /explain` - Explain code snippets
- `POST /review` - Code review and suggestions

**Use Case**: IDE integration, code review automation, developer tools

---

## Build System

All examples use the same Python-based build system with progress indicators:

### Features
- **Cross-platform**: Works on Linux, macOS, Windows (WSL)
- **Progress Indicators**: Spinning animation during compilation
- **Informative Messages**: Clear status updates and next steps
- **Optimized Builds**: Compilation completes in reasonable time

### Build Output
```
Building Application Server...
  Compiling... (this may take some time due to large header-only libraries)
  Progress: ✓ Compilation complete!

✓ Application Server built successfully!

Building CLI Client...
  Compiling... (this may take some time due to large header-only libraries)
  Progress: ✓ Compilation complete!

✓ CLI Client built successfully!

✓ Build Complete! (2/2 succeeded)
```

---

## Configuration

Each example includes a `config.txt` file:

```ini
# LlamaClient Configuration
LLAMA_SERVER_HOST=127.0.0.1
LLAMA_SERVER_PORT=8080
LLAMA_BIN_PATH=~/llama.cpp/build/bin/llama-server
LLAMA_MODEL_PATH=~/llama.cpp/models/my-model.gguf
```

### Environment Variables
Configuration supports shell expansion (Linux/macOS):
- `~` expands to home directory
- `${VAR}` expands environment variables
- Absolute and relative paths supported

---

## Prerequisites

### 1. llama.cpp Installation
```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
cmake -B build
cmake --build build
```

### 2. Model Download
Download a compatible GGUF model:
```bash
cd llama.cpp/models
# Example: Download a small model
wget https://huggingface.co/.../model.gguf
```

### 3. Dependencies
- **C++17 compiler**: g++ 9+, clang++ 10+, MSVC 2019+
- **Python 3**: For build scripts
- **llama.cpp**: Built and configured

---

## Running Examples

### Step 1: Configure
```bash
cd chatbot  # Or any example
nano config.txt  # Edit paths to match your llama.cpp installation
```

### Step 2: Start llama-server
```bash
# Option A: Manual start
~/llama.cpp/build/bin/llama-server \
  -m ~/llama.cpp/models/my-model.gguf \
  --host 127.0.0.1 \
  --port 8080

# Option B: Let app-server start it (if LLAMA_BIN_PATH configured)
# Just run ./bin/server and it will start llama-server automatically
```

### Step 3: Build and Run
```bash
python3 build.py
./bin/server      # Start application server
./bin/client chat "Hello!"  # In another terminal
```

---

## Customization

### Creating Your Own Application

Use `llama-app-generator` to create custom applications:

```bash
cd /path/to/llama-app-generator
./bin/llama-app-generator my-app /path/to/output

cd /path/to/output/my-app
# Edit src/server.cpp - customize handleRequest()
# Edit src/client.cpp - add custom commands
python3 build.py
```

### Modifying Examples

1. **Server Logic**: Edit `src/server.cpp` → `handleRequest()` method
2. **Client Commands**: Edit `src/client.cpp` → `main()` function
3. **Configuration**: Add parameters to `config.txt`
4. **Rebuild**: `python3 build.py`

---

## Performance Notes

### Build Times
- **Generator**: < 1 second (no heavy headers)
- **Examples**: 5-10 seconds per .cpp file

### Why Examples Build Slower
Each example includes large header-only libraries:
- `json.hpp`: 24,765 lines (nlohmann/json)
- `httplib.h`: 11,911 lines (cpp-httplib)
- **Total**: ~36,676 lines parsed per compilation

This is expected for header-only libraries. The progress indicators keep you informed during compilation.

---

## Troubleshooting

### "Connection refused" Error
```
Error: Cannot connect to llama-server at 127.0.0.1:8080
```
**Solution**: Start llama-server first (see "Running Examples" above)

### "Port already in use" Error
```
Error: Failed to start server on port 8081
```
**Solution**: Change port in server code or stop conflicting service

### Build Errors
```
fatal error: json.hpp: No such file or directory
```
**Solution**: Ensure you're building from the example directory (templates are copied during generation)

---

## License

All examples are licensed under **Apache License 2.0**.

Each example includes:
- `LICENSE` - Full Apache 2.0 license text
- `NOTICE` - Namespace attribution and copyright notice

**Copyright © 2025 Pooria Yousefi**

See individual example directories for full license and attribution.

---

## Contact

**Author**: Pooria Yousefi  
**Email**: pooriayousefi@aol.com  
**GitHub**: https://github.com/pooriayousefi

For bugs, questions, or contributions, please open an issue on GitHub.

---

## Acknowledgments

These examples were developed with assistance from **Claude Sonnet 4.5 (Preview)**.

See `../ACKNOWLEDGMENTS.md` for full attribution and third-party library credits.
